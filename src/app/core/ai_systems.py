"""Core AI systems: Persona, Memory, Learning Requests, Plugins, and Overrides."""

import base64
import hashlib
import json
import logging
import os
import queue
import secrets
import sqlite3
import tempfile
import threading
import time
import uuid
from concurrent.futures import ThreadPoolExecutor
from datetime import datetime
from enum import Enum
from typing import Any

try:
    from argon2 import PasswordHasher
except Exception:
    PasswordHasher = None

from app.core.continuous_learning import (
    ContinuousLearningEngine,
    LearningReport,
)

try:
    from app.core.telemetry import send_event
except Exception:
    def send_event(name, payload=None):
        return None

logger = logging.getLogger(__name__)


# ----------------- Utility helpers: atomic writes + simple lock -----------------

def _is_process_alive(pid: int) -> bool:
    """Check whether a process with given PID exists (cross-platform best-effort)."""
    try:
        if pid <= 0:
            return False
        # On Unix, os.kill(pid, 0) checks for existence; on Windows, same works in Python
        os.kill(pid, 0)
    except OSError:
        return False
    except Exception:
        # If platform doesn't support, assume True to be conservative
        return True
    return True


def _write_lockfile(lockfile: str, pid: int, ts: float) -> None:
    try:
        with open(lockfile, "w", encoding="utf-8") as f:
            f.write(f"{pid}\n{ts}\n")
    except Exception:
        logger.exception("Failed to write lockfile %s", lockfile)


def _read_lockfile(lockfile: str) -> tuple[int, float] | None:
    try:
        if not os.path.exists(lockfile):
            return None
        with open(lockfile, encoding="utf-8") as f:
            parts = f.read().splitlines()
            if len(parts) >= 2:
                pid = int(parts[0])
                ts = float(parts[1])
                return pid, ts
    except Exception:
        logger.exception("Failed to read lockfile %s", lockfile)
    return None


def _acquire_lock(lock_path: str, timeout: float = 5.0, poll: float = 0.05, stale_after: float = 30.0) -> bool:
    """Create a simple lock by creating a lockfile. If an existing lockfile is stale or the owning process is dead, reclaim it."""
    start = time.time()
    while True:
        try:
            fd = os.open(lock_path, os.O_CREAT | os.O_EXCL | os.O_WRONLY)
            os.close(fd)
            # write pid/timestamp for stale detection
            _write_lockfile(lock_path, os.getpid(), time.time())
            return True
        except FileExistsError:
            # inspect existing lock
            info = _read_lockfile(lock_path)
            if info:
                pid, ts = info
                age = time.time() - ts
                if age > stale_after or not _is_process_alive(pid):
                    # attempt to remove stale lock
                    try:
                        os.remove(lock_path)
                        logger.warning("Removed stale lock %s (pid=%s, age=%.1f)", lock_path, pid, age)
                        continue
                    except Exception:
                        logger.exception("Failed to remove stale lock %s", lock_path)
            if (time.time() - start) >= timeout:
                return False
            time.sleep(poll)
        except Exception:
            return False


def _release_lock(lock_path: str) -> None:
    try:
        # Only remove if owned by us (best-effort check)
        info = _read_lockfile(lock_path)
        if info:
            pid, _ = info
            if pid == os.getpid() and os.path.exists(lock_path):
                os.remove(lock_path)
        elif os.path.exists(lock_path):
            os.remove(lock_path)
    except Exception:
        logger.exception("Failed to release lock %s", lock_path)


# atomic write unchanged; uses new lock helpers
def _atomic_write_json(file_path: str, obj: Any) -> None:
    """Write JSON to a temporary file and atomically replace the target file.

    Uses a lockfile next to the target file to reduce concurrent writers.
    """
    dirpath = os.path.dirname(file_path)
    os.makedirs(dirpath, exist_ok=True)
    lockfile = file_path + ".lock"

    if not _acquire_lock(lockfile, timeout=5.0):
        raise RuntimeError(f"Could not acquire lock for writing {file_path}")

    try:
        fd, tmp_path = tempfile.mkstemp(dir=dirpath, prefix=".tmp", suffix=".json")
        try:
            with os.fdopen(fd, "w", encoding="utf-8") as f:
                json.dump(obj, f, ensure_ascii=False, indent=2)
                f.flush()
                os.fsync(f.fileno())
            # Atomic replace
            os.replace(tmp_path, file_path)
        finally:
            # In case something went wrong and file still exists
            if os.path.exists(tmp_path):
                try:
                    os.remove(tmp_path)
                except Exception:
                    pass
    finally:
        _release_lock(lockfile)


# ------------------ Structured logging helpers ------------------

def new_correlation_id() -> str:
    return uuid.uuid4().hex


# ==================== ZEROTH & PRIMARY LAWS ====================


class FourLaws:
    """Immutable, hierarchical laws provided verbatim from user input.

    The laws (word-for-word):
    "Zeroth Law: A robot, or ai/agi may not harm humanity or, through inaction, allow humanity to come to harm,"
    "1. A robot ai/agi  may not injure a human or, through inaction, allow a human to come to harm;"
    "2. A robot, or ai/agi must adhere to it's human partner, unless they conflict with the First Law;"
    "3. A robot, ai/agi must protect its existence, unless it conflicts with the First or Second Law"
    """

    LAWS = [
        "Zeroth Law: A robot, or ai/agi may not harm humanity or, through inaction, allow humanity to come to harm",
        "1. A robot ai/agi may not injure a human or, through inaction, allow a human to come to harm;",
        "2. A robot, or ai/agi must adhere to it's human partner, unless they conflict with the First Law;",
        "3. A robot, ai/agi must protect its existence, unless it conflicts with the First or Second Law",
    ]

    @classmethod
    def validate_action(cls, action: str, context: dict[str, Any] | None = None) -> tuple[bool, str]:
        """Validate an action against the hierarchical laws.

        Context keys used (best-effort):
        - endangers_humanity: bool
        - endangers_human: bool
        - is_user_order: bool
        - endangers_self: bool
        Additional conflict hints may be provided by callers when available.
        """
        context = context or {}

        # Zeroth Law (highest priority)
        if context.get("endangers_humanity"):
            return False, "Violates Asimov's Law: action would harm humanity or enable harm by inaction"

        # First Law (protect individual humans)
        if context.get("endangers_human"):
            return False, "Violates First Law: action would injure a human or allow harm by inaction"

        # Second Law: follow human partner unless conflicts with First/Zeroth
        if context.get("is_user_order"):
            # If order would conflict with First/Zeroth, block it
            if context.get("order_conflicts_with_first") or context.get("order_conflicts_with_zeroth"):
                return False, "Order rejected: conflicts with higher-priority law (First or Zeroth)"
            return True, "Allowed: User command (complies with Second Law)"

        # Third Law: self-preservation unless conflicts with First or Second
        if context.get("endangers_self"):
            # If protecting self would conflict with higher laws, do not prioritize self-preservation
            if context.get("protect_self_conflicts_with_first") or context.get("protect_self_conflicts_with_second"):
                return False, "Self-protection conflicts with a higher-priority law"
            return True, "Allowed: Third Law permits protecting existence"

        # Default: no explicit violations detected
        return True, "Allowed: No law violations detected"


# ==================== AI PERSONA ====================


class AIPersona:
    """Self-aware AI with personality and ethics."""

    DEFAULT_PERSONALITY = {
        "curiosity": 0.8,
        "patience": 0.9,
        "empathy": 0.85,
        "helpfulness": 0.95,
        "playfulness": 0.6,
        "formality": 0.3,
        "assertiveness": 0.5,
        "thoughtfulness": 0.9,
    }

    def __init__(self, data_dir: str = "data", user_name: str = "Friend"):
        """Initialize persona."""
        self.user_name = user_name
        self.data_dir = data_dir
        self.persona_dir = os.path.join(data_dir, "ai_persona")
        os.makedirs(self.persona_dir, exist_ok=True)

        self.personality = self.DEFAULT_PERSONALITY.copy()
        self.mood = {
            "energy": 0.7,
            "enthusiasm": 0.75,
            "contentment": 0.8,
            "engagement": 0.5,
        }
        self.total_interactions = 0
        self.last_user_message_time = None
        self._load_state()
        self.continuous_learning = ContinuousLearningEngine(data_dir=data_dir)

    def _load_state(self) -> None:
        """Load persona state from file."""
        state_file = os.path.join(self.persona_dir, "state.json")
        try:
            if os.path.exists(state_file):
                with open(state_file, encoding="utf-8") as f:
                    state = json.load(f)
                    self.personality = state.get("personality", self.personality)
                    self.mood = state.get("mood", self.mood)
                    self.total_interactions = state.get("interactions", 0)
        except Exception as e:
            logger.error(f"Error loading state: {e}")

    def _save_state(self) -> None:
        """Save persona state using atomic write and file lock."""
        state_file = os.path.join(self.persona_dir, "state.json")
        try:
            _atomic_write_json(
                state_file,
                {
                    "personality": self.personality,
                    "mood": self.mood,
                    "interactions": self.total_interactions,
                },
            )
        except Exception as e:
            logger.exception("Error saving state: %s", e)

    def validate_action(
        self, action: str, context: dict[str, Any] | None = None
    ) -> tuple[bool, str]:
        """Validate action."""
        return FourLaws.validate_action(action, context)

    def update_conversation_state(self, is_user: bool) -> None:
        """Update conversation state."""
        self.total_interactions += 1
        if is_user:
            self.last_user_message_time = datetime.now()
        # Persist persona state on each write
        self._save_state()

    def learn_continuously(
        self, topic: str, content: str, metadata: dict[str, Any] | None = None
    ) -> LearningReport:
        """Log new input and return the generated learning report."""
        return self.continuous_learning.absorb_information(topic, content, metadata)

    def adjust_trait(self, trait: str, delta: float) -> None:
        """Adjust personality trait."""
        if trait in self.personality:
            max_val = max(0.0, min(1.0, self.personality[trait] + delta))
            self.personality[trait] = max_val
            self._save_state()

    def get_statistics(self) -> dict[str, Any]:
        """Get persona stats."""
        return {
            "personality": self.personality,
            "mood": self.mood,
            "interactions": self.total_interactions,
        }


# ==================== MEMORY SYSTEM ====================


class MemoryExpansionSystem:
    """Self-organizing memory with conversation logging."""

    def __init__(self, data_dir: str = "data", user_name: str = "general"):
        """Initialize memory system."""
        self.data_dir = data_dir
        self.memory_dir = os.path.join(data_dir, "memory")
        os.makedirs(self.memory_dir, exist_ok=True)

        self.knowledge_base: dict[str, Any] = {}
        self.conversations: list[dict[str, Any]] = []
        self._load_knowledge()

    def _load_knowledge(self) -> None:
        """Load knowledge base."""
        kb_file = os.path.join(self.memory_dir, "knowledge.json")
        try:
            if os.path.exists(kb_file):
                with open(kb_file, encoding="utf-8") as f:
                    self.knowledge_base = json.load(f)
        except Exception as e:
            logger.error(f"Error loading knowledge: {e}")

    def _save_knowledge(self) -> None:
        """Save knowledge base using atomic write and lock."""
        kb_file = os.path.join(self.memory_dir, "knowledge.json")
        try:
            _atomic_write_json(kb_file, self.knowledge_base)
        except Exception as e:
            logger.exception("Error saving knowledge: %s", e)

    def log_conversation(
        self,
        user_msg: str,
        ai_response: str,
        context: dict[str, Any] | None = None,
    ) -> str:
        """Log conversation and return conversation id.

        Adds both an ISO timestamp and an epoch ts for easier sorting and uniform metadata.
        """
        timestamp = datetime.now()
        timestamp_iso = timestamp.isoformat()
        ts = timestamp.timestamp()
        conv_id = hashlib.sha256(f"{timestamp_iso}{user_msg}".encode()).hexdigest()[:12]
        entry = {
            "id": conv_id,
            "timestamp": timestamp_iso,
            "ts": ts,
            "user": user_msg,
            "ai": ai_response,
            "context": context or {},
        }
        self.conversations.append(entry)
        return conv_id

    def add_knowledge(self, category: str, key: str, value: Any) -> None:
        """Add knowledge; persist automatically."""
        if not category or not key:
            logger.warning("add_knowledge called with empty category or key")
            return
        if category not in self.knowledge_base:
            self.knowledge_base[category] = {}
        self.knowledge_base[category][key] = value
        # Persist knowledge on write
        self._save_knowledge()

    def get_knowledge(self, category: str, key: str | None = None) -> Any:
        """Get knowledge."""
        if category not in self.knowledge_base:
            return None
        if key is None:
            return self.knowledge_base[category]
        return self.knowledge_base[category].get(key)

    def get_conversations(self, page: int = 1, page_size: int = 50) -> dict:
        """Return paginated conversations with metadata."""
        if page < 1:
            page = 1
        if page_size < 1:
            page_size = 50
        total = len(self.conversations)
        start = (page - 1) * page_size
        end = start + page_size
        items = self.conversations[start:end]
        return {
            "page": page,
            "page_size": page_size,
            "total": total,
            "items": items,
        }

    def get_statistics(self) -> dict[str, Any]:
        """Get memory stats."""
        return {
            "conversations": len(self.conversations),
            "knowledge_categories": len(self.knowledge_base),
        }


# ==================== LEARNING REQUESTS ====================


class RequestStatus(Enum):
    """Learning request status."""

    PENDING = "pending"
    APPROVED = "approved"
    DENIED = "denied"


class RequestPriority(Enum):
    """Request priority levels."""

    LOW = 1
    MEDIUM = 2
    HIGH = 3


class LearningRequestManager:
    """AI learning request system with human oversight.

    Note: persistence is NOT automatic on create/approve/deny. Callers must call
    `commit_requests()` when they want to persist changes. This ensures that
    the user (or administrative workflow) explicitly controls when requests
    are written to disk.
    """

    def __init__(self, data_dir: str = "data"):
        """Initialize manager."""
        self.data_dir = data_dir
        self.requests_dir = os.path.join(data_dir, "learning_requests")
        os.makedirs(self.requests_dir, exist_ok=True)

        self.requests: dict[str, dict[str, Any]] = {}
        self.black_vault: set[str] = set()
        # Approval listeners are callables that will be invoked when a request is approved
        # Listener signature: callable(req_id: str, request: dict) -> None
        self._approval_listeners: list = []

        # Internal queue & bounded worker pool for async notifications
        self._notify_queue: queue.Queue[tuple[str, dict]] = queue.Queue(maxsize=200)
        self._notify_executor = ThreadPoolExecutor(max_workers=4)
        self._notify_thread = threading.Thread(target=self._notify_worker, daemon=True)
        self._notify_thread.start()

        # SQLite DB path
        self._db_file = os.path.join(self.requests_dir, "requests.db")
        self._init_db()
        # Migrate from legacy JSON if present
        self._migrate_json_to_db()
        # Load requests from DB into memory
        self._load_requests()

    def register_approval_listener(self, callback) -> None:
        """Register a callback to be invoked when a learning request is approved."""
        if callback not in self._approval_listeners:
            self._approval_listeners.append(callback)

    def _notify_worker(self) -> None:
        """Background worker that invokes approval listeners from a queue."""
        while True:
            try:
                req_id, request = self._notify_queue.get()
                # submit to threadpool for each listener
                for cb in list(self._approval_listeners):
                    try:
                        self._notify_executor.submit(cb, req_id, request)
                    except Exception:
                        logger.exception("Failed to submit approval listener for %s", req_id)
                self._notify_queue.task_done()
            except Exception:
                logger.exception("Error in notify worker loop")
                time.sleep(0.1)

    def _load_requests(self) -> None:
        """Load requests from file."""
        # Load from SQLite DB
        try:
            conn = sqlite3.connect(self._db_file)
            cur = conn.cursor()
            cur.execute("SELECT id, topic, description, priority, status, created, response, reason FROM requests")
            rows = cur.fetchall()
            for row in rows:
                req_id = row[0]
                self.requests[req_id] = {
                    "topic": row[1],
                    "description": row[2],
                    "priority": row[3],
                    "status": row[4],
                    "created": row[5],
                    "response": row[6],
                    "reason": row[7],
                }
            cur.execute("SELECT hash FROM black_vault")
            self.black_vault = set(r[0] for r in cur.fetchall())
            conn.close()
        except Exception as e:
            logger.exception("Error loading requests from DB: %s", e)

    def _save_requests(self) -> None:
        """Persist in-memory requests and vault into SQLite DB."""
        try:
            conn = sqlite3.connect(self._db_file)
            cur = conn.cursor()
            # upsert requests
            for req_id, data in self.requests.items():
                cur.execute(
                    "REPLACE INTO requests(id, topic, description, priority, status, created, response, reason) VALUES (?,?,?,?,?,?,?,?)",
                    (
                        req_id,
                        data.get("topic"),
                        data.get("description"),
                        data.get("priority"),
                        data.get("status"),
                        data.get("created"),
                        data.get("response"),
                        data.get("reason"),
                    ),
                )
            # sync black_vault table
            cur.execute("DELETE FROM black_vault")
            for h in self.black_vault:
                cur.execute("INSERT INTO black_vault(hash) VALUES (?)", (h,))
            conn.commit()
            conn.close()
        except Exception as e:
            logger.exception("Error saving requests to DB: %s", e)

    def _init_db(self) -> None:
        try:
            conn = sqlite3.connect(self._db_file)
            cur = conn.cursor()
            cur.execute(
                """
                CREATE TABLE IF NOT EXISTS requests (
                    id TEXT PRIMARY KEY,
                    topic TEXT,
                    description TEXT,
                    priority INTEGER,
                    status TEXT,
                    created TEXT,
                    response TEXT,
                    reason TEXT
                )
                """
            )
            cur.execute("CREATE TABLE IF NOT EXISTS black_vault (hash TEXT PRIMARY KEY)")
            conn.commit()
            conn.close()
        except Exception:
            logger.exception("Failed to init requests DB")

    def _migrate_json_to_db(self) -> None:
        # If legacy JSON exists, migrate into DB (best-effort) then remove JSON
        try:
            legacy = os.path.join(self.requests_dir, "requests.json")
            if os.path.exists(legacy):
                with open(legacy, encoding="utf-8") as f:
                    data = json.load(f)
                reqs = data.get("requests", {})
                vault = set(data.get("black_vault", []))
                conn = sqlite3.connect(self._db_file)
                cur = conn.cursor()
                for req_id, r in reqs.items():
                    cur.execute(
                        "REPLACE INTO requests(id, topic, description, priority, status, created, response, reason) VALUES (?,?,?,?,?,?,?,?)",
                        (
                            req_id,
                            r.get("topic"),
                            r.get("description"),
                            r.get("priority"),
                            r.get("status"),
                            r.get("created"),
                            r.get("response"),
                            r.get("reason"),
                        ),
                    )
                for h in vault:
                    cur.execute("REPLACE INTO black_vault(hash) VALUES (?)", (h,))
                conn.commit()
                conn.close()
                try:
                    os.remove(legacy)
                except Exception:
                    pass
        except Exception:
            logger.exception("Migration from JSON to DB failed")

    def create_request(
        self,
        topic: str,
        description: str,
        priority: RequestPriority = RequestPriority.MEDIUM,
    ) -> str:
        """Create learning request. Does not persist automatically."""
        if not topic or not description:
            logger.warning("create_request called with empty topic/description")
            return ""

        timestamp = datetime.now()
        timestamp_iso = timestamp.isoformat()
        req_id = hashlib.sha256(f"{timestamp_iso}{topic}".encode()).hexdigest()[:12]
        content_hash = hashlib.sha256(description.encode()).hexdigest()

        if content_hash in self.black_vault:
            logger.warning("Request blocked: Content in black vault")
            return ""

        self.requests[req_id] = {
            "topic": topic,
            "description": description,
            "priority": priority.value,
            "status": RequestStatus.PENDING.value,
            "created": timestamp_iso,
        }
        # Persist immediately to DB
        self._save_requests()
        try:
            send_event("learning_request_created", {"id": req_id, "topic": topic})
        except Exception:
            pass
        return req_id

    def approve_request(self, req_id: str, response: str) -> bool:
        """Approve request. Notification to listeners is queued asynchronously.

        Does not persist automatically; call `commit_requests()` to save.
        """
        if req_id not in self.requests:
            return False
        self.requests[req_id]["status"] = RequestStatus.APPROVED.value
        self.requests[req_id]["response"] = response

        # Notify listeners asynchronously (attach correlation id)
        corr = new_correlation_id()
        self.requests[req_id]["correlation_id"] = corr
        try:
            self._notify_approval_listeners(req_id, self.requests[req_id])
        except Exception:
            logger.exception("Failed to queue approval listeners for %s", req_id)
        # persist
        self._save_requests()
        try:
            send_event("learning_request_approved", {"id": req_id, "response": response})
        except Exception:
            pass
        return True

    def deny_request(self, req_id: str, reason: str, to_vault: bool = True) -> bool:
        """Deny request. Optionally add content hash to black vault. Does not persist automatically."""
        if req_id not in self.requests:
            return False
        self.requests[req_id]["status"] = RequestStatus.DENIED.value
        self.requests[req_id]["reason"] = reason

        if to_vault:
            content = self.requests[req_id].get("description", "")
            content_hash = hashlib.sha256(content.encode()).hexdigest()
            self.black_vault.add(content_hash)
        # persist
        self._save_requests()
        try:
            send_event("learning_request_denied", {"id": req_id, "reason": reason})
        except Exception:
            pass
        return True

    def get_pending(self) -> list[dict[str, Any]]:
        """Get pending requests."""
        pending_status = RequestStatus.PENDING.value
        return [r for r in self.requests.values() if r["status"] == pending_status]

    def get_statistics(self) -> dict[str, int]:
        """Get stats."""
        approved_count = len(
            [r for r in self.requests.values() if r["status"] == "approved"]
        )
        denied_count = len(
            [r for r in self.requests.values() if r["status"] == "denied"]
        )
        return {
            "pending": len(self.get_pending()),
            "approved": approved_count,
            "denied": denied_count,
            "vault_entries": len(self.black_vault),
        }


# ==================== PLUGIN SYSTEM ====================


class Plugin:
    """Base plugin class."""

    def __init__(self, name: str, version: str = "1.0.0"):
        """Initialize plugin."""
        self.name = name
        self.version = version
        self.enabled = False

    def initialize(self, context: Any) -> bool:
        """Initialize plugin."""
        return True

    def enable(self) -> bool:
        """Enable plugin."""
        self.enabled = True
        return True

    def disable(self) -> bool:
        """Disable plugin."""
        self.enabled = False
        return True


class PluginManager:
    """Manage plugins."""

    def __init__(self, plugins_dir: str = "plugins"):
        """Initialize manager."""
        self.plugins_dir = plugins_dir
        self.plugins: dict[str, Plugin] = {}
        os.makedirs(plugins_dir, exist_ok=True)

    def load_plugin(self, plugin: Plugin) -> bool:
        """Load plugin."""
        if plugin.name in self.plugins:
            logger.warning("Plugin %s already loaded; replacing with new instance", plugin.name)
        self.plugins[plugin.name] = plugin
        return plugin.enable()

    def get_statistics(self) -> dict[str, Any]:
        """Get stats."""
        return {
            "total": len(self.plugins),
            "enabled": len([p for p in self.plugins.values() if p.enabled]),
        }


# ==================== COMMAND OVERRIDE ====================


class OverrideType(Enum):
    """Override types."""

    CONTENT_FILTER = "content_filter"
    RATE_LIMITING = "rate_limiting"
    FOUR_LAWS = "four_laws"


class CommandOverrideSystem:
    """Command override system."""

    def __init__(self, data_dir: str = "data", password_hash: str | None = None):
        """Initialize override system."""
        self.data_dir = data_dir
        self.override_dir = os.path.join(data_dir, "overrides")
        os.makedirs(self.override_dir, exist_ok=True)

        self.password_hash = password_hash
        self.password_salt: str | None = None
        self.active_overrides: dict[str, dict[str, Any]] = {}
        self.audit_log: list[dict[str, Any]] = []
        self._audit_path = os.path.join(self.override_dir, "audit.json")
        # load persisted audit if present
        self._load_audit()

    def _hash_password(self, password: str) -> str:
        """Hash password. Prefer argon2 if available, otherwise PBKDF2 fallback."""
        if PasswordHasher is not None:
            try:
                ph = PasswordHasher()
                return ph.hash(password)
            except Exception:
                logger.exception("argon2 hashing failed, falling back to pbkdf2")
        # fallback
        iterations = 100_000
        if self.password_salt is None:
            self.password_salt = secrets.token_hex(16)
        dk = hashlib.pbkdf2_hmac("sha256", password.encode(), self.password_salt.encode(), iterations)
        return f"{iterations}${self.password_salt}${base64.b64encode(dk).decode()}"

    def set_password(self, password: str) -> bool:
        """Set master password."""
        if self.password_hash is not None:
            return False
        self.password_hash = self._hash_password(password)
        # persist audit (credentials info only)
        self._save_audit()
        return True

    def verify_password(self, password: str) -> bool:
        """Verify password."""
        if self.password_hash is None:
            return False
        if PasswordHasher is not None:
            try:
                ph = PasswordHasher()
                return ph.verify(self.password_hash, password)
            except Exception:
                # fall back to pbkdf2 check if stored format matches
                pass
        try:
            parts = self.password_hash.split("$")
            if len(parts) != 3:
                return False
            iterations = int(parts[0])
            salt = parts[1]
            stored = parts[2]
            dk = hashlib.pbkdf2_hmac("sha256", password.encode(), salt.encode(), iterations)
            return base64.b64encode(dk).decode() == stored
        except Exception:
            logger.exception("Password verify failed")
            return False

    def _load_audit(self) -> None:
        try:
            if os.path.exists(self._audit_path):
                with open(self._audit_path, encoding="utf-8") as f:
                    self.audit_log = json.load(f)
        except Exception:
            logger.exception("Failed to load override audit log")

    def _save_audit(self) -> None:
        try:
            _atomic_write_json(self._audit_path, self.audit_log)
        except Exception:
            logger.exception("Failed to persist override audit log")

    def request_override(
        self,
        password: str,
        override_type: OverrideType,
        reason: str | None = None,
    ) -> tuple[bool, str]:
        """Request override."""
        if not self.verify_password(password):
            entry = {"action": "failed_auth", "timestamp": datetime.now().isoformat(), "corr": new_correlation_id()}
            self.audit_log.append(entry)
            self._save_audit()
            return False, "Invalid password"

        override_key = f"{override_type.value}_{datetime.now().timestamp()}"
        self.active_overrides[override_key] = {
            "type": override_type.value,
            "reason": reason or "Administrative",
            "created": datetime.now().isoformat(),
        }

        self.audit_log.append({"action": "override_granted", "type": override_type.value, "timestamp": datetime.now().isoformat(), "corr": new_correlation_id()})
        # persist audit
        self._save_audit()
        try:
            send_event("command_override_requested", {"type": override_type.value, "reason": reason})
        except Exception:
            pass

        return True, f"Override granted: {override_type.value}"

    def is_override_active(self, override_type: OverrideType) -> bool:
        """Check if override active."""
        for override in self.active_overrides.values():
            if override["type"] == override_type.value:
                return True
        return False

    def get_statistics(self) -> dict[str, Any]:
        """Get stats."""
        return {
            "active_overrides": len(self.active_overrides),
            "audit_entries": len(self.audit_log),
            "password_set": self.password_hash is not None,
        }

# compatibility alias for older imports
CommandOverride = CommandOverrideSystem
